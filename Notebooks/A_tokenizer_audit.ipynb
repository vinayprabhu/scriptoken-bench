{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1yfvWsB88mR"
      },
      "source": [
        "# Purpose\n",
        "The purpose of this notebook is to download all the SOTA LLM tokenizers\n",
        "\n",
        "- Most important link is the one for [Scripts.txt](https://www.unicode.org/Public/17.0.0/ucd/Scripts.txt)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgsDDufR1_w8",
        "outputId": "371a6a81-f7bf-4bcd-8878-b2b742c1365f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-419160616.py:15: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
            "  set_matplotlib_formats('retina')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from google.colab import userdata\n",
        "%matplotlib inline\n",
        "\n",
        "from scipy.linalg import block_diag\n",
        "# Don't do linear algebra in Python without these two lines\n",
        "np.set_printoptions(suppress=True)\n",
        "from collections import Counter\n",
        "from IPython.display import set_matplotlib_formats\n",
        "set_matplotlib_formats('retina')\n",
        "\n",
        "%precision 3\n",
        "#############################################\n",
        "import sys\n",
        "import importlib\n",
        "importlib.reload(sys)\n",
        "#######################\n",
        "from google.colab import drive\n",
        "drive.flush_and_unmount()\n",
        "import os\n",
        "drive.mount('/gdrive', force_remount=True)\n",
        "# Enter your own proj_dir here\n",
        "proj_dir='/gdrive/My Drive/Blog/Code/tokens/Data/'\n",
        "os.chdir(proj_dir)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title  Install all the needed libs and authenticate wrt HF hub\n",
        "!pip install tiktoken transformers sentencepiece huggingface_hub -q\n",
        "import tiktoken\n",
        "import transformers\n",
        "from transformers import AutoTokenizer\n",
        "from huggingface_hub import login\n",
        "from huggingface_hub.utils import disable_progress_bars\n",
        "from google.colab import userdata\n",
        "\n",
        "# SILENCE DOWNLOADS & LOGGING\n",
        "disable_progress_bars()\n",
        "transformers.utils.logging.set_verbosity_error()\n",
        "transformers.utils.logging.disable_progress_bar()\n",
        "\n",
        "# AUTHENTICATE : WE ARE USING GATED MODELS.\n",
        "token_hf = userdata.get('HF_TOKEN')\n",
        "login(token=token_hf)"
      ],
      "metadata": {
        "id": "I-xDSCpzAwnb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Import all the SoTA tokenizers from the marquee models:\n",
        "import random\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer\n",
        "import tiktoken\n",
        "\n",
        "# SOTA Model Registry with Hand-Populated Types\n",
        "# BPE: Typically Byte-level (Tiktoken or HF implementation)\n",
        "# SentencePiece: Used by Google/Meta for specific multilingual/multimodal architectures\n",
        "SOTA_CONFIG = {\n",
        "    \"GPT-OSS\": {\"id\": \"o200k_base\", \"type\": \"BPE (Tiktoken)\"},\n",
        "    \"Llama-4\": {\"id\": \"meta-llama/Llama-4-Scout-17B-16E-Instruct\", \"type\": \"BPE (Tiktoken)\"},\n",
        "    \"Mistral\": {\"id\": \"mistralai/Mistral-Large-3-675B-Instruct-2512\", \"type\": \"BPE (Tekken)\"},\n",
        "    \"Gemma-3\": {\"id\": \"google/gemma-3-270m-it\", \"type\": \"SentencePiece\"},\n",
        "    \"GLM-4\": {\"id\": \"THUDM/glm-4-9b-chat\", \"type\": \"BPE (Tiktoken)\"},\n",
        "    \"Qwen-2.5\": {\"id\": \"Qwen/Qwen2.5-72B-Instruct\", \"type\": \"BPE\"},\n",
        "    \"DeepSeek-V3\": {\"id\": \"deepseek-ai/DeepSeek-V3\", \"type\": \"BPE\"},\n",
        "    \"Phi-4\": {\"id\": \"microsoft/phi-4\", \"type\": \"BPE\"},\n",
        "    \"RNJ-1\": {\"id\": \"EssentialAI/rnj-1-instruct\", \"type\": \"SentencePiece\"},\n",
        "    \"OLMo-3\": {\"id\": \"allenai/OLMo-3-1125-32B\", \"type\": \"BPE\"}\n",
        "}\n",
        "\n",
        "def get_tokenizer_stats(name, config):\n",
        "    try:\n",
        "        if name == \"GPT-OSS\":\n",
        "            tok = tiktoken.get_encoding(config[\"id\"])\n",
        "            vocab_size = tok.n_vocab\n",
        "            sample_indices = random.sample(range(vocab_size), 10)\n",
        "            samples = [tok.decode([i]).strip() for i in sample_indices]\n",
        "        else:\n",
        "            tok = AutoTokenizer.from_pretrained(config[\"id\"], trust_remote_code=True)\n",
        "            vocab_size = len(tok)\n",
        "            vocab = tok.get_vocab()\n",
        "            sample_keys = random.sample(list(vocab.keys()), 10)\n",
        "            samples = [str(k).replace('Ġ', ' ').replace(' ', ' ').strip() for k in sample_keys]\n",
        "\n",
        "        return {\n",
        "            \"Model\": name,\n",
        "            \"Type\": config[\"type\"],\n",
        "            \"Vocab Size\": vocab_size,\n",
        "            \"Samples\": \", \".join([f\"'{s}'\" for s in samples[:5]])\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {\"Model\": name, \"Type\": config[\"type\"], \"Vocab Size\": 0, \"Samples\": \"Loading Error\"}\n",
        "\n",
        "# 1. Collect Data\n",
        "data = [get_tokenizer_stats(name, cfg) for name, cfg in SOTA_CONFIG.items()]\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# 2. Add \"All_combined\" Row\n",
        "total_tokens = df[\"Vocab Size\"].sum()\n",
        "combined_row = pd.DataFrame([{\n",
        "    \"Model\": \"All_combined\",\n",
        "    \"Type\": \"N/A\",\n",
        "    \"Vocab Size\": total_tokens,\n",
        "    \"Samples\": \"Total Aggregate Vocabulary\"\n",
        "}])\n",
        "df = pd.concat([df, combined_row], ignore_index=True)\n",
        "\n",
        "# 3. Generate LaTeX\n",
        "latex_table = df.to_latex(\n",
        "    index=False,\n",
        "    caption=\"Comparative Analysis of SOTA Tokenizers (2026)\",\n",
        "    label=\"tab:tokenizer_comparison\",\n",
        "    column_format=\"|l|l|r|p{5cm}|\",\n",
        "    escape=True,\n",
        "    longtable=False\n",
        ")\n",
        "\n",
        "print(\"--- CONSOLE VIEW ---\")\n",
        "print(df.to_string())\n",
        "\n",
        "print(\"\\n--- LATEX CODE ---\")\n",
        "print(latex_table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DC9I4IK1bdJK",
        "outputId": "a6632860-4e2b-41af-9e44-d7321aeb6edc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- CONSOLE VIEW ---\n",
            "           Model            Type  Vocab Size                                                                    Samples\n",
            "0        GPT-OSS  BPE (Tiktoken)      200019                                     'Arb', 'переп', 'notify', 'तान', 'zak'\n",
            "1        Llama-4  BPE (Tiktoken)      201135                             'ÄĲáº·c', 'odziel', 'une', 'Consider', 'weich'\n",
            "2        Mistral    BPE (Tekken)      131072                                'skirts', 'Empty', '\"),Ċ', 'ìĿ¸ë¯¼', 'hatt'\n",
            "3        Gemma-3   SentencePiece      262145                      'setOnAction', '▁Exist', '▁Fah', '教会', '<unused3749>'\n",
            "4          GLM-4  BPE (Tiktoken)      151343  'b' Provision'', 'b' pr\\xc3\\xb3xima'', 'b' newVal'', 'b'yii'', 'b'nicas''\n",
            "5       Qwen-2.5             BPE      151665                              'hacking', 'datable', '-ring', 'plung', 'ĉti'\n",
            "6    DeepSeek-V3             BPE      128815                      'Industrial', 'Revenue', 'å¾ĹäºĨ', 'ãģłãģĳãģ§', 'å¾ĭ'\n",
            "7          Phi-4             BPE      100352                                    'rein', 'audio', 'perse', '.Gray', '%-'\n",
            "8          RNJ-1   SentencePiece      128256                          'transition', 'req', 'ÎºÎ±', 'aisy', 'Ð»ÑİÐ´ÐµÐ¹'\n",
            "9         OLMo-3             BPE      100278                     'hasta', 'acquiring', 'lu', 'XmlElement', 'amendments'\n",
            "10  All_combined             N/A     1555080                                                 Total Aggregate Vocabulary\n",
            "\n",
            "--- LATEX CODE ---\n",
            "\\begin{table}\n",
            "\\caption{Comparative Analysis of SOTA Tokenizers (2026)}\n",
            "\\label{tab:tokenizer_comparison}\n",
            "\\begin{tabular}{|l|l|r|p{5cm}|}\n",
            "\\toprule\n",
            "Model & Type & Vocab Size & Samples \\\\\n",
            "\\midrule\n",
            "GPT-OSS & BPE (Tiktoken) & 200019 & 'Arb', 'переп', 'notify', 'तान', 'zak' \\\\\n",
            "Llama-4 & BPE (Tiktoken) & 201135 & 'ÄĲáº·c', 'odziel', 'une', 'Consider', 'weich' \\\\\n",
            "Mistral & BPE (Tekken) & 131072 & 'skirts', 'Empty', '\"),Ċ', 'ìĿ¸ë¯¼', 'hatt' \\\\\n",
            "Gemma-3 & SentencePiece & 262145 & 'setOnAction', '▁Exist', '▁Fah', '教会', '<unused3749>' \\\\\n",
            "GLM-4 & BPE (Tiktoken) & 151343 & 'b' Provision'', 'b' pr\\textbackslash xc3\\textbackslash xb3xima'', 'b' newVal'', 'b'yii'', 'b'nicas'' \\\\\n",
            "Qwen-2.5 & BPE & 151665 & 'hacking', 'datable', '-ring', 'plung', 'ĉti' \\\\\n",
            "DeepSeek-V3 & BPE & 128815 & 'Industrial', 'Revenue', 'å¾ĹäºĨ', 'ãģłãģĳãģ§', 'å¾ĭ' \\\\\n",
            "Phi-4 & BPE & 100352 & 'rein', 'audio', 'perse', '.Gray', '\\%-' \\\\\n",
            "RNJ-1 & SentencePiece & 128256 & 'transition', 'req', 'ÎºÎ±', 'aisy', 'Ð»ÑİÐ´ÐµÐ¹' \\\\\n",
            "OLMo-3 & BPE & 100278 & 'hasta', 'acquiring', 'lu', 'XmlElement', 'amendments' \\\\\n",
            "All\\_combined & N/A & 1555080 & Total Aggregate Vocabulary \\\\\n",
            "\\bottomrule\n",
            "\\end{tabular}\n",
            "\\end{table}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unicode 17 CODEPOINT Look up table"
      ],
      "metadata": {
        "id": "1wG42jfezd_U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -L \"https://www.unicode.org/Public/17.0.0/ucd/Scripts.txt\" -o Scripts-17.0.0.txt\n",
        "\n",
        "################################################\n",
        "\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "import unicodedata\n",
        "\n",
        "def get_unicode_script_dataframe(url,file_output):\n",
        "    # Comprehensive Category Meanings from Wikipedia\n",
        "    category_map = {\n",
        "        # L: Letter\n",
        "        'Lu': 'Letter, uppercase',\n",
        "        'Ll': 'Letter, lowercase',\n",
        "        'Lt': 'Letter, titlecase',\n",
        "        'Lm': 'Letter, modifier',\n",
        "        'Lo': 'Letter, other',\n",
        "        # M: Mark\n",
        "        'Mn': 'Mark, nonspacing',\n",
        "        'Mc': 'Mark, spacing combining',\n",
        "        'Me': 'Mark, enclosing',\n",
        "        # N: Number\n",
        "        'Nd': 'Number, decimal digit',\n",
        "        'Nl': 'Number, letter',\n",
        "        'No': 'Number, other',\n",
        "        # P: Punctuation\n",
        "        'Pc': 'Punctuation, connector',\n",
        "        'Pd': 'Punctuation, dash',\n",
        "        'Ps': 'Punctuation, open',\n",
        "        'Pe': 'Punctuation, close',\n",
        "        'Pi': 'Punctuation, initial quote',\n",
        "        'Pf': 'Punctuation, final quote',\n",
        "        'Po': 'Punctuation, other',\n",
        "        # S: Symbol\n",
        "        'Sm': 'Symbol, math',\n",
        "        'Sc': 'Symbol, currency',\n",
        "        'Sk': 'Symbol, modifier',\n",
        "        'So': 'Symbol, other',\n",
        "        # Z: Separator\n",
        "        'Zs': 'Separator, space',\n",
        "        'Zl': 'Separator, line',\n",
        "        'Zp': 'Separator, paragraph',\n",
        "        # C: Other\n",
        "        'Cc': 'Other, control',\n",
        "        'Cf': 'Other, format',\n",
        "        'Cs': 'Other, surrogate',\n",
        "        'Co': 'Other, private use',\n",
        "        'Cn': 'Other, not assigned'\n",
        "    }\n",
        "\n",
        "    # Fetch the Scripts.txt file\n",
        "    response = requests.get(url)\n",
        "    lines = response.text.splitlines()\n",
        "\n",
        "    rows = []\n",
        "\n",
        "    for line in lines:\n",
        "        if not line or line.startswith('#'):\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            parts = line.split(';')\n",
        "            codepoint_part = parts[0].strip()\n",
        "            rest = parts[1].split('#')\n",
        "            script_name = rest[0].strip()\n",
        "            meta = rest[1].strip().split()\n",
        "\n",
        "            # The category code is the first element after the '#'\n",
        "            category_code = meta[0]\n",
        "            # Capture the remaining text in the comment as notes\n",
        "            notes = \" \".join(meta[1:])\n",
        "\n",
        "            if '..' in codepoint_part:\n",
        "                start_hex, end_hex = codepoint_part.split('..')\n",
        "            else:\n",
        "                start_hex = end_hex = codepoint_part\n",
        "\n",
        "            start_val = int(start_hex, 16)\n",
        "            end_val = int(end_hex, 16)\n",
        "\n",
        "            for cp in range(start_val, end_val + 1):\n",
        "                rows.append({\n",
        "                    'codepoint_hex': f\"{cp:04X}\",\n",
        "                    'script_name': script_name,\n",
        "                    'category': category_code,\n",
        "                    'notes': notes,\n",
        "                    'category_meaning': category_map.get(category_code, \"Unknown\")\n",
        "                })\n",
        "        except (IndexError, ValueError):\n",
        "            continue\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "    df.to_csv(file_output, sep='\\t', index=False)\n",
        "    print(f\"Successfully saved {len(df)} rows to {file_output}\")\n",
        "    return df\n",
        "\n",
        "url = \"https://www.unicode.org/Public/17.0.0/ucd/Scripts.txt\"\n",
        "df_unicode17 = get_unicode_script_dataframe(url,'df_unicode_17.tsv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IwaMGNOEwTwT",
        "outputId": "5c346644-0d91-4d97-dd47-adc0c35f2e4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  187k    0  187k    0     0   249k      0 --:--:-- --:--:-- --:--:--  249k\n",
            "Successfully saved 159866 rows to df_unicode_17.tsv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_unicode17.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "BoooXJqMz9pu",
        "outputId": "aea28410-48d0-40d6-be39-94670be7d5e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  codepoint_hex script_name category                                notes  \\\n",
              "0          0000      Common       Cc  [32] <control-0000>..<control-001F>   \n",
              "1          0001      Common       Cc  [32] <control-0000>..<control-001F>   \n",
              "2          0002      Common       Cc  [32] <control-0000>..<control-001F>   \n",
              "3          0003      Common       Cc  [32] <control-0000>..<control-001F>   \n",
              "4          0004      Common       Cc  [32] <control-0000>..<control-001F>   \n",
              "\n",
              "  category_meaning  \n",
              "0   Other, control  \n",
              "1   Other, control  \n",
              "2   Other, control  \n",
              "3   Other, control  \n",
              "4   Other, control  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cbc26c3f-dcf2-49f2-87ce-a52ffc97ce3f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>codepoint_hex</th>\n",
              "      <th>script_name</th>\n",
              "      <th>category</th>\n",
              "      <th>notes</th>\n",
              "      <th>category_meaning</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0000</td>\n",
              "      <td>Common</td>\n",
              "      <td>Cc</td>\n",
              "      <td>[32] &lt;control-0000&gt;..&lt;control-001F&gt;</td>\n",
              "      <td>Other, control</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0001</td>\n",
              "      <td>Common</td>\n",
              "      <td>Cc</td>\n",
              "      <td>[32] &lt;control-0000&gt;..&lt;control-001F&gt;</td>\n",
              "      <td>Other, control</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0002</td>\n",
              "      <td>Common</td>\n",
              "      <td>Cc</td>\n",
              "      <td>[32] &lt;control-0000&gt;..&lt;control-001F&gt;</td>\n",
              "      <td>Other, control</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0003</td>\n",
              "      <td>Common</td>\n",
              "      <td>Cc</td>\n",
              "      <td>[32] &lt;control-0000&gt;..&lt;control-001F&gt;</td>\n",
              "      <td>Other, control</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0004</td>\n",
              "      <td>Common</td>\n",
              "      <td>Cc</td>\n",
              "      <td>[32] &lt;control-0000&gt;..&lt;control-001F&gt;</td>\n",
              "      <td>Other, control</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cbc26c3f-dcf2-49f2-87ce-a52ffc97ce3f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-cbc26c3f-dcf2-49f2-87ce-a52ffc97ce3f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-cbc26c3f-dcf2-49f2-87ce-a52ffc97ce3f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_unicode17"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Helper functions to perform the script audit\n",
        "import re\n",
        "import random\n",
        "import pandas as pd\n",
        "import tiktoken\n",
        "from typing import Dict, List, Set, Union\n",
        "from transformers import AutoTokenizer, PreTrainedTokenizerBase\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 1. THIS WAS THE SOTA REGISTRY (DATE: Jan 2026)\n",
        "# ---------------------------------------------------------\n",
        "# SOTA_CONFIG = {\n",
        "#     \"GPT-OSS\": {\"id\": \"o200k_base\", \"type\": \"BPE (Tiktoken)\"},\n",
        "#     \"Llama-4\": {\"id\": \"meta-llama/Llama-4-Scout-17B-16E-Instruct\", \"type\": \"BPE (Tiktoken)\"},\n",
        "#     \"Mistral\": {\"id\": \"mistralai/Mistral-Large-3-675B-Instruct-2512\", \"type\": \"BPE (Tekken)\"},\n",
        "#     \"Gemma-3\": {\"id\": \"google/gemma-3-270m-it\", \"type\": \"SentencePiece\"},\n",
        "#     \"GLM-4\": {\"id\": \"THUDM/glm-4-9b-chat\", \"type\": \"BPE (Tiktoken/Custom)\"},\n",
        "#     \"Qwen-2.5\": {\"id\": \"Qwen/Qwen2.5-72B-Instruct\", \"type\": \"BPE\"},\n",
        "#     \"DeepSeek-V3\": {\"id\": \"deepseek-ai/DeepSeek-V3\", \"type\": \"BPE\"},\n",
        "#     \"Phi-4\": {\"id\": \"microsoft/phi-4\", \"type\": \"BPE\"},\n",
        "#     \"RNJ-1\": {\"id\": \"EssentialAI/rnj-1-instruct\", \"type\": \"SentencePiece\"},\n",
        "#     \"OLMo-3\": {\"id\": \"allenai/OLMo-3-1125-32B\", \"type\": \"BPE\"}\n",
        "# }\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 2. THE AUDIT ENGINE CLASS\n",
        "# ---------------------------------------------------------\n",
        "class TokenScriptAuditor:\n",
        "    def __init__(self, scripts_txt_path: str = \"Scripts.txt\"):\n",
        "        self.cp_to_script = self._load_scripts(scripts_txt_path)\n",
        "        self.ignore_from_mix = {\"Common\", \"Unknown\", \"Inherited\"}\n",
        "\n",
        "    def _load_scripts(self, path: str) -> Dict[int, str]:\n",
        "        cp_map = {}\n",
        "        if not os.path.exists(path):\n",
        "            raise FileNotFoundError(f\"Missing Unicode Scripts file: {path}\")\n",
        "\n",
        "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                line = line.split(\"#\")[0].strip()\n",
        "                if not line or \";\" not in line: continue\n",
        "                range_part, script = line.split(\";\")\n",
        "                script = script.strip().split()[0]\n",
        "                if \"..\" in range_part:\n",
        "                    start, end = range_part.split(\"..\")\n",
        "                    for cp in range(int(start, 16), int(end, 16) + 1):\n",
        "                        cp_map[cp] = script\n",
        "                else:\n",
        "                    cp_map[int(range_part, 16)] = script\n",
        "        return cp_map\n",
        "\n",
        "    def identify_script(self, text: str) -> str:\n",
        "        if not text: return \"EMPTY\"\n",
        "        scripts_found = {self.cp_to_script.get(ord(c), \"Unknown\") for c in text}\n",
        "        meaningful = sorted(list(scripts_found - self.ignore_from_mix))\n",
        "\n",
        "        if not meaningful:\n",
        "            return sorted(list(scripts_found))[0] if scripts_found else \"Unknown\"\n",
        "        if len(meaningful) == 1:\n",
        "            return meaningful[0]\n",
        "\n",
        "        # New Logic: Mixed_Script1_Script2\n",
        "        return \"Mixed_\" + \"_\".join(meaningful[:2])\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 3. HELPER FUNCTIONS\n",
        "# ---------------------------------------------------------\n",
        "def audit_model_vocab(name: str, config: dict, auditor: TokenScriptAuditor) -> pd.DataFrame:\n",
        "    print(f\"--- Starting Audit: {name} ({config['id']}) ---\")\n",
        "\n",
        "    # Load Tokenizer\n",
        "    if name == \"GPT-OSS\":\n",
        "        tokenizer = tiktoken.get_encoding(config[\"id\"])\n",
        "        indices = range(tokenizer.n_vocab)\n",
        "        is_tiktoken = True\n",
        "    else:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(config[\"id\"], trust_remote_code=True)\n",
        "        id_to_token = {v: k for k, v in tokenizer.get_vocab().items()}\n",
        "        indices = sorted(id_to_token.keys())\n",
        "        is_tiktoken = False\n",
        "\n",
        "    rows = []\n",
        "    for i in indices:\n",
        "        try:\n",
        "            if is_tiktoken:\n",
        "                raw_token = str(tokenizer.decode_single_token_bytes(i))\n",
        "                decoded = tokenizer.decode([i])\n",
        "            else:\n",
        "                raw_token = id_to_token[i]\n",
        "                decoded = tokenizer.decode([i], skip_special_tokens=False)\n",
        "\n",
        "            # Labeling\n",
        "            if re.match(r\"^<.*>$\", raw_token):\n",
        "                script_label = \"SPECIAL_TOKEN\"\n",
        "            else:\n",
        "                script_label = auditor.identify_script(decoded)\n",
        "        except:\n",
        "            decoded, script_label = \"N/A\", \"BYTE_FRAGMENT\"\n",
        "            raw_token = id_to_token[i] if not is_tiktoken else f\"idx_{i}\"\n",
        "\n",
        "        rows.append([name, i, raw_token, decoded, script_label])\n",
        "\n",
        "    return pd.DataFrame(rows, columns=[\"model_tokenizer_name\", \"tokenizer_token_index\", \"token\", \"decoded_token\", \"script\"])\n"
      ],
      "metadata": {
        "id": "QJA4oTD5vccY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "auditor = TokenScriptAuditor(\"./scripts_unicode_17.txt/Scripts.txt\")\n",
        "\n",
        "list_all = []\n",
        "total_token_count = 0\n",
        "\n",
        "for name, config in SOTA_CONFIG.items():\n",
        "    df = audit_model_vocab(name, config, auditor)\n",
        "    list_all.append(df)\n",
        "    total_token_count += len(df)\n",
        "\n",
        "# Combine All\n",
        "df_comb = pd.concat(list_all, ignore_index=True)\n",
        "\n",
        "# Summary Row Logic\n",
        "print(f\"\\nFinal Audit Complete.\")\n",
        "print(f\"Total Tokens Processed: {total_token_count:,}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5O-xEaT3v7np",
        "outputId": "78248e1b-228c-4de4-cd78-2f53c9803fec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Audit: GPT-OSS (o200k_base) ---\n",
            "--- Starting Audit: Llama-4 (meta-llama/Llama-4-Scout-17B-16E-Instruct) ---\n",
            "--- Starting Audit: Mistral (mistralai/Mistral-Large-3-675B-Instruct-2512) ---\n",
            "--- Starting Audit: Gemma-3 (google/gemma-3-270m-it) ---\n",
            "--- Starting Audit: GLM-4 (THUDM/glm-4-9b-chat) ---\n",
            "--- Starting Audit: Qwen-2.5 (Qwen/Qwen2.5-72B-Instruct) ---\n",
            "--- Starting Audit: DeepSeek-V3 (deepseek-ai/DeepSeek-V3) ---\n",
            "--- Starting Audit: Phi-4 (microsoft/phi-4) ---\n",
            "--- Starting Audit: RNJ-1 (EssentialAI/rnj-1-instruct) ---\n",
            "--- Starting Audit: OLMo-3 (allenai/OLMo-3-1125-32B) ---\n",
            "\n",
            "Final Audit Complete.\n",
            "Total Tokens Processed: 1,555,080\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Something doesn't seem right about GLM"
      ],
      "metadata": {
        "id": "IWot3S-oCm6L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_comb.loc[(df_comb.model_tokenizer_name=='GLM-4') ].script.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "1MMURPDVB0Bs",
        "outputId": "6f25197e-77d9-417f-82ba-4d1fd409271c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "script\n",
              "BYTE_FRAGMENT    151329\n",
              "SPECIAL_TOKEN        11\n",
              "Latin                 3\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>script</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>BYTE_FRAGMENT</th>\n",
              "      <td>151329</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SPECIAL_TOKEN</th>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Latin</th>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "def audit_glm4_vocab(model_id: str, auditor) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    GLM-4 specific audit function.\n",
        "    Handles byte-level vocabulary and custom ChatGLM tokenization logic.\n",
        "    \"\"\"\n",
        "    print(f\"--- Loading GLM-4 Tokenizer: {model_id} ---\")\n",
        "\n",
        "    # GLM-4 requires trust_remote_code=True for its custom tiktoken-based implementation\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
        "\n",
        "    # GLM-4 vocab keys are 'bytes' objects\n",
        "    vocab = tokenizer.get_vocab()\n",
        "    id_to_token = {v: k for k, v in vocab.items()}\n",
        "    indices = sorted(id_to_token.keys())\n",
        "\n",
        "    # Pre-compile byte-pattern to avoid TypeError: cannot use string pattern on bytes\n",
        "    # Matches patterns like b'<|user|>', b'<|endoftext|>', etc.\n",
        "    special_byte_pattern = re.compile(b\"^<.*>$\")\n",
        "\n",
        "    rows = []\n",
        "    model_name = model_id.split(\"/\")[-1]\n",
        "\n",
        "    for i in indices:\n",
        "        raw_token_bytes = id_to_token[i] # This is a 'bytes' object\n",
        "\n",
        "        try:\n",
        "            # 1. Decode for the 'decoded_token' column (human readable)\n",
        "            decoded = tokenizer.decode([i], skip_special_tokens=False)\n",
        "\n",
        "            # 2. Determine Script\n",
        "            # Check against the byte-regex for special tokens\n",
        "            if special_byte_pattern.match(raw_token_bytes):\n",
        "                script_label = \"SPECIAL_TOKEN\"\n",
        "            elif not decoded:\n",
        "                script_label = \"BYTE_FRAGMENT\"\n",
        "            else:\n",
        "                script_label = auditor.identify_script(decoded)\n",
        "\n",
        "        except Exception:\n",
        "            decoded = \"N/A\"\n",
        "            script_label = \"BYTE_FRAGMENT\"\n",
        "\n",
        "        rows.append([\n",
        "            model_name,\n",
        "            i,\n",
        "            str(raw_token_bytes), # Store as string representation of bytes b'...'\n",
        "            decoded,\n",
        "            script_label\n",
        "        ])\n",
        "\n",
        "    df = pd.DataFrame(rows, columns=[\n",
        "        \"model_tokenizer_name\", \"tokenizer_token_index\",\n",
        "        \"token\", \"decoded_token\", \"script\"\n",
        "    ])\n",
        "\n",
        "    return df\n",
        "\n",
        "#######################\n",
        "df_glm4 = audit_glm4_vocab(\"THUDM/glm-4-9b-chat\", auditor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sS6Y5Z8iCH0e",
        "outputId": "50e75213-29c1-4a35-972d-3b12dbe43dac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Loading GLM-4 Tokenizer: THUDM/glm-4-9b-chat ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_glm4.script.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 930
        },
        "id": "D3lo40FODDDM",
        "outputId": "909c9753-7bf7-497b-e2bf-2f3e6f8098d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "script\n",
              "Latin                      100587\n",
              "Han                         28513\n",
              "Cyrillic                     9741\n",
              "Common                       7998\n",
              "Arabic                       1972\n",
              "Greek                         829\n",
              "Hangul                        565\n",
              "Hiragana                      486\n",
              "Katakana                      352\n",
              "Mixed_Han_Hiragana            115\n",
              "Thai                           58\n",
              "Devanagari                     27\n",
              "Hebrew                         22\n",
              "Mixed_Han_Latin                16\n",
              "Inherited                      15\n",
              "BYTE_FRAGMENT                  14\n",
              "Bengali                         9\n",
              "SPECIAL_TOKEN                   5\n",
              "Tamil                           5\n",
              "Khmer                           3\n",
              "Mixed_Greek_Latin               3\n",
              "Malayalam                       2\n",
              "Mixed_Hiragana_Katakana         2\n",
              "Braille                         2\n",
              "Unknown                         1\n",
              "Mixed_Han_Katakana              1\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>script</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Latin</th>\n",
              "      <td>100587</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Han</th>\n",
              "      <td>28513</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Cyrillic</th>\n",
              "      <td>9741</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Common</th>\n",
              "      <td>7998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Arabic</th>\n",
              "      <td>1972</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Greek</th>\n",
              "      <td>829</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Hangul</th>\n",
              "      <td>565</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Hiragana</th>\n",
              "      <td>486</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Katakana</th>\n",
              "      <td>352</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Mixed_Han_Hiragana</th>\n",
              "      <td>115</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Thai</th>\n",
              "      <td>58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Devanagari</th>\n",
              "      <td>27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Hebrew</th>\n",
              "      <td>22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Mixed_Han_Latin</th>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Inherited</th>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>BYTE_FRAGMENT</th>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Bengali</th>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SPECIAL_TOKEN</th>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Tamil</th>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Khmer</th>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Mixed_Greek_Latin</th>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Malayalam</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Mixed_Hiragana_Katakana</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Braille</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Unknown</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Mixed_Han_Katakana</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Neat!\n",
        "lst_mod=list(df_comb.model_tokenizer_name.unique())\n",
        "lst_mod.remove('GLM-4')\n",
        "df_temp=df_comb[df_comb.model_tokenizer_name.isin(lst_mod)]\n",
        "df_final=pd.concat([df_temp,df_glm4])\n",
        "\n",
        "# Export to TSV\n",
        "df_final.to_csv(\"df_tokka_bench_2026.tsv\", sep='\\t',index=False)\n",
        "df_final.model_tokenizer_name.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429
        },
        "id": "yIfGgh1NDooq",
        "outputId": "bde8efbc-6e51-4f0a-f054-8dc3a850484c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "model_tokenizer_name\n",
              "Gemma-3          262145\n",
              "Llama-4          201135\n",
              "GPT-OSS          200019\n",
              "Qwen-2.5         151665\n",
              "glm-4-9b-chat    151343\n",
              "Mistral          131072\n",
              "DeepSeek-V3      128815\n",
              "RNJ-1            128256\n",
              "Phi-4            100352\n",
              "OLMo-3           100278\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>model_tokenizer_name</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Gemma-3</th>\n",
              "      <td>262145</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Llama-4</th>\n",
              "      <td>201135</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GPT-OSS</th>\n",
              "      <td>200019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Qwen-2.5</th>\n",
              "      <td>151665</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>glm-4-9b-chat</th>\n",
              "      <td>151343</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Mistral</th>\n",
              "      <td>131072</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DeepSeek-V3</th>\n",
              "      <td>128815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RNJ-1</th>\n",
              "      <td>128256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Phi-4</th>\n",
              "      <td>100352</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>OLMo-3</th>\n",
              "      <td>100278</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_final.script.unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYwBL6GLNrOV",
        "outputId": "ddd8d45c-1725-449c-e313-eeedcf06642c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Common', 'Latin', 'Cyrillic', 'Arabic', 'Devanagari', 'Georgian',\n",
              "       'Hebrew', 'Armenian', 'Malayalam', 'Greek', 'Bengali', 'Han',\n",
              "       'Gujarati', 'Tamil', 'Kannada', 'Telugu', 'Thai', 'Hangul',\n",
              "       'Inherited', 'Hiragana', 'Katakana', 'Gurmukhi', 'Sinhala',\n",
              "       'Khmer', 'Myanmar', 'Mixed_Han_Hiragana', 'Oriya',\n",
              "       'Mixed_Han_Latin', 'Unknown', 'Tibetan', 'Braille',\n",
              "       'Mixed_Han_Katakana', 'Mixed_Cyrillic_Latin', 'Lao',\n",
              "       'BYTE_FRAGMENT', 'Ethiopic', 'Thaana', 'SPECIAL_TOKEN',\n",
              "       'Mixed_Hiragana_Katakana', 'Mixed_Greek_Latin',\n",
              "       'Mixed_Hiragana_Latin', 'Syriac', 'Mixed_Katakana_Latin',\n",
              "       'Cherokee', 'Ogham', 'Ol_Chiki', 'Tifinagh', 'Cuneiform', 'Coptic',\n",
              "       'Canadian_Aboriginal', 'Vai', 'Bopomofo', 'Egyptian_Hieroglyphs',\n",
              "       'Yi', 'Mongolian', 'Javanese', 'Old_Turkic', 'Kaithi', 'Tai_Le',\n",
              "       'Meetei_Mayek', 'Nko', 'Tai_Viet', 'Bamum', 'New_Tai_Lue', 'Runic',\n",
              "       'Mandaic', 'Phags_Pa', 'Tai_Tham', 'Balinese', 'Buginese',\n",
              "       'Gothic', 'Limbu', 'Sundanese', 'Batak', 'Lepcha', 'Phoenician',\n",
              "       'Glagolitic', 'Samaritan', 'Lisu', 'Cham', 'Old_Persian',\n",
              "       'Inscriptional_Parthian', 'Modi'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_final.script.nunique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5YfuFEVqN7ht",
        "outputId": "cd9a6411-0ca8-4911-80fe-60cf5b14cae4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "83"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_final.script.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "o2EZ1hqzOS9I",
        "outputId": "c45a30fa-87ad-4b77-af88-257b84760085"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "script\n",
              "Latin          1051942\n",
              "Han             137815\n",
              "Cyrillic         85253\n",
              "Common           79131\n",
              "Arabic           43366\n",
              "                ...   \n",
              "Batak                1\n",
              "Lisu                 1\n",
              "Cham                 1\n",
              "Old_Persian          1\n",
              "Modi                 1\n",
              "Name: count, Length: 83, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>script</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Latin</th>\n",
              "      <td>1051942</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Han</th>\n",
              "      <td>137815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Cyrillic</th>\n",
              "      <td>85253</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Common</th>\n",
              "      <td>79131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Arabic</th>\n",
              "      <td>43366</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Batak</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Lisu</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Cham</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Old_Persian</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Modi</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>83 rows × 1 columns</p>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "mount_file_id": "1OokzHx8QscskgTD-TjsSB9VZkk93CHEc",
      "authorship_tag": "ABX9TyP/TnTPOcr6jbkv7svEd2pv"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}